{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fredafal/anaconda3/bin/python3\n",
      "/Users/fredafal/anaconda3/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install py4j\n",
    "print(sys.executable)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark1 = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x113291a20>\n",
      "<pyspark.sql.session.SparkSession object at 0x113291a20>\n"
     ]
    }
   ],
   "source": [
    "print(spark1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "sc=SparkContext(master=\"local[4]\")\n",
    "lst=np.random.randint(0,10,20)\n",
    "A=sc.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 8, 7, 9, 6, 2, 3, 9, 2, 9], [8, 8, 1, 6, 8, 5, 2, 4, 4, 8]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc=SparkContext(master=\"local[2]\")\n",
    "A = sc.parallelize(lst)\n",
    "A.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.reduce(lambda x,y: x if x > y else y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'These are some of the best Macintosh computers ever'.split(' ')\n",
    "wordRDD = sc.parallelize(words)\n",
    "wordRDD.reduce(lambda w,v: w if len(w)>len(v) else v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 6, 3, 9, 9, 6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.filter(lambda x:x%3==0 and x!=0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 2, 2, 2, 4, 4, 6, 6, 8, 8, 8, 8, 8]), (1, [1, 3, 5, 7, 9, 9, 9])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=A.groupBy(lambda x:x%2).collect()\n",
    "sorted([(x, sorted(y)) for (x, y) in result])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "df = spark.read.json(\"/usr/local/Cellar/apache-spark/3.0.0/libexec/examples/src/main/resources/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(age)=24.5)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg({\"age\": \"avg\"}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "|Alice| 10|    80|\n",
      "+-----+---+------+\n",
      "\n",
      "+-----+---+------+\n",
      "| name|age|height|\n",
      "+-----+---+------+\n",
      "|Alice|  5|    80|\n",
      "+-----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "df = sc.parallelize([ \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=5, height=80), \\\n",
    "    Row(name='Alice', age=10, height=80)]).toDF()\n",
    "df.dropDuplicates().show()\n",
    "df.dropDuplicates(['name', 'height']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('height', 'bigint')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'all', 'my', 'name', 'is', 'Tom', 'I', 'am', 'originally', 'from', 'Australia']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence1 = \"Hi all, my name is Tom...I am originally from Australia.\"\n",
    "def solution(sentence):\n",
    "    for p in \"!?',;.\":\n",
    "        sentence = sentence.replace(p, ' ')\n",
    "    words = sentence.split()\n",
    "    return words\n",
    "\n",
    "print(solution(sentence1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 64, 49, 81, 36, 4, 9, 81, 4, 81, 64, 64, 1, 36, 64, 25, 4, 16, 16, 64]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=A.map(lambda x:x*x)\n",
    "B.collect()\n",
    "B.histogram([x for x in range(0,100,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 2 µs, total: 12 µs\n",
      "Wall time: 675 µs\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(master=\"local[2]\")\n",
    "%time\n",
    "from math import cos\n",
    "def taketime(x):\n",
    "    [cos(j) for j in range(100)]\n",
    "    return cos(x)\n",
    "\n",
    "\n",
    "rdd1 = sc.parallelize(range(1000000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n",
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "taketime(2)\n",
    "%time\n",
    "interim = rdd1.map(lambda x: taketime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n",
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 8.11 µs\n",
      "output = -0.2887054679684593\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#interim.collect()\n",
    "%time\n",
    "print('output =',interim.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "print(rdd1.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(lst):\n",
    "    \"\"\"\n",
    "    Bubble sort function\n",
    "    :param lst: lst of unsorted integers\n",
    "    \"\"\"\n",
    "    # Traverse through all list elements\n",
    "    for i in range(len(lst)):\n",
    "        # Last i elements are already in place\n",
    "        for j in range(0, len(lst) - i - 1):\n",
    "            # Traverse the list from 0 to size of lst - i - 1\n",
    "            # Swap if the element found is greater than the next element\n",
    "            if lst[j] > lst[j + 1]:\n",
    "                lst[j], lst[j + 1] = lst[j + 1], lst[j]\n",
    "            #print(lst)\n",
    "\n",
    "\n",
    "# Driver code to test above\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    lst = [3, 35, 23, 7, 11, 2, 20, 1]\n",
    "    bubble_sort(lst)  # Calling bubble sort function\n",
    "\n",
    "    print(\"Sorted list is: \", lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[[1, 1, 0, 2, 0], [1, 1, 0, 2, 0], [1, 1, 0, 0, 0], [0, 0, 0, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Number of Islands\n",
    "Given a 2d grid map of '1's (land) and '0's (water), count the number of islands.\n",
    "An island is surrounded by water and is formed by connecting adjacent lands horizontally or vertically.\n",
    "You may assume all four edges of the grid are all surrounded by water.\n",
    "Input:  [\n",
    "            ['1','1','1','1','0'],\n",
    "            ['1','1','0','1','0'],\n",
    "            ['1','1','0','0','0'],\n",
    "            ['0','0','0','0','0']\n",
    "        ]\n",
    "Output: 1\n",
    "Input:  [\n",
    "            ['1','1','0','0','0'],\n",
    "            ['1','1','0','0','0'],\n",
    "            ['0','0','1','0','0'],\n",
    "            ['0','0','0','1','1']\n",
    "        ]\n",
    "Output: 3\n",
    "'''\n",
    "\n",
    "in_array = [\n",
    "            ['1','1','0','1','0'],\n",
    "            ['1','1','0','1','0'],\n",
    "            ['1','1','0','0','0'],\n",
    "            ['0','0','0','1','1']\n",
    "        ]\n",
    "\n",
    "islands = [[0 for j in range(len(in_array[0]))] for i in range(len(in_array))]\n",
    "\n",
    "def prev_island(i,j):\n",
    "    prev_i = i - 1\n",
    "    prev_j = j - 1\n",
    "    island = 0\n",
    "    if prev_i != -1:\n",
    "        island = islands[prev_i][j]\n",
    "    if not island:\n",
    "        island = islands[i][prev_j]\n",
    "    \n",
    "    return island\n",
    "    \n",
    "def main():\n",
    "    isl_cnt = 0\n",
    "    for i in range(len(in_array)):\n",
    "        for j in range(len(in_array[0])):\n",
    "            if in_array[i][j] == '1':\n",
    "                island = prev_island(i,j)\n",
    "                if island:\n",
    "                    islands[i][j] = island\n",
    "                else:\n",
    "                    isl_cnt += 1\n",
    "                    islands[i][j] = isl_cnt\n",
    "\n",
    "    print(isl_cnt)\n",
    "    print(islands)\n",
    "    \n",
    "main()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The Power Set\n",
    "The power set of a set is the set of all its subsets.\n",
    "Write a function that, given a set, generates its power set.\n",
    "Input: [1, 2, 3, 4]\n",
    "Output: [\n",
    "[],\n",
    "[1], [2], [3], [4],\n",
    "[1,2], [1,3], [1,4], [2,3], [2,4], [3,4],\n",
    "[1,2,3], [1,2,4], [1,3,4], [2,3,4],\n",
    "[1,2,3,4]\n",
    "]\n",
    "* You may also use a list or array to represent a set.\n",
    "=========================================\n",
    "Simple recursive combinations algorithm.\n",
    "    Time Complexity:    O(Sum(C(I, N)))     , sum of all combinations between 0 and N = C(0, N) + C(1, N) + ... + C(N, N)\n",
    "    Space Complexity:   O(Sum(C(I, N)))     , this is for the result array, if we print the number then the space complexity will be O(N) (because of the recursive stack)\n",
    "I did not solve this one.\n",
    "'''\n",
    "\n",
    "in_list = [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "  \n",
    "def power_set(arr):\n",
    "    result = []\n",
    "    combinations(result, arr, [], 0)\n",
    "    return result\n",
    "\n",
    "# result, arr and taken are the same references always\n",
    "def combinations(result, arr, taken, pos):\n",
    "    result.append([arr[i] for i in taken]) # create the current combination\n",
    "    n = len(arr)\n",
    "    if n == pos:\n",
    "        return\n",
    "\n",
    "    # start from the last position (don't need duplicates)\n",
    "    for i in range(pos, n):\n",
    "        taken.append(i)\n",
    "        combinations(result, arr, taken, i + 1)\n",
    "        del taken[-1] # return to the old state\n",
    "\n",
    "print(power_set(in_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# initialization of spark context\n",
    "# conf = SparkConf().setAppName(appName).setMaster(master) \n",
    "sc = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .config(\"spark.some.config.option\", \"some-value\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# read data from HDFS, as a result we get RDD of lines\n",
    "linesRDD = sc.read.text(\"votes.test\" ) # \"hdfs://...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(sc.__class__)\n",
    "dir(sc)\n",
    "print(linesRDD.__class__)\n",
    "#dir(linesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New context and imports \n",
    "# from: https://www.guru99.com/pyspark-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "print(pyspark.__version__)\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "nums = sc.parallelize([1,2,3,4])\n",
    "nums.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print('%i ' % (num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[4] at readRDDFromFile at PythonRDD.scala:262\n",
      "[Row(name='Smith', age=29), Row(name='John', age=19), Row(name='Henry', age=50), Row(name='Adam', age=35)]\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sqlContext.createDataFrame(ppl)\n",
    "list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]\n",
    "rdd = sc.parallelize(list_p)\n",
    "print(rdd)\n",
    "ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "print(ppl.top(10))\n",
    "DF_ppl = sqlContext.createDataFrame(ppl)\n",
    "\n",
    "DF_ppl.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/var/folders/6d/br2_wqvd3kb2_5n3bf60fkpw0000gp/T/spark-8911c858-8c67-40f9-933a-03cae1ed2ffc/userFiles-af71261c-68e8-4e38-9147-0f2e9a46ff23/adult_data.csv\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "url = \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "f = SparkFiles.get(\"adult_data.csv\")\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- workclass: string (nullable = true)\n",
      " |-- fnlwgt: integer (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- educational-num: integer (nullable = true)\n",
      " |-- marital-status: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- relationship: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- capital-gain: integer (nullable = true)\n",
      " |-- capital-loss: integer (nullable = true)\n",
      " |-- hours-per-week: integer (nullable = true)\n",
      " |-- native-country: string (nullable = true)\n",
      " |-- income: string (nullable = true)\n",
      "\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|x  |age|workclass|fnlwgt|education   |educational-num|marital-status    |occupation       |relationship|race |gender|capital-gain|capital-loss|hours-per-week|native-country|income|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "|1  |25 |Private  |226802|11th        |7              |Never-married     |Machine-op-inspct|Own-child   |Black|Male  |0           |0           |40            |United-States |<=50K |\n",
      "|2  |38 |Private  |89814 |HS-grad     |9              |Married-civ-spouse|Farming-fishing  |Husband     |White|Male  |0           |0           |50            |United-States |<=50K |\n",
      "|3  |28 |Local-gov|336951|Assoc-acdm  |12             |Married-civ-spouse|Protective-serv  |Husband     |White|Male  |0           |0           |40            |United-States |>50K  |\n",
      "|4  |44 |Private  |160323|Some-college|10             |Married-civ-spouse|Machine-op-inspct|Husband     |Black|Male  |7688        |0           |40            |United-States |>50K  |\n",
      "|5  |18 |?        |103497|Some-college|10             |Never-married     |?                |Own-child   |White|Female|0           |0           |30            |United-States |<=50K |\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(f), header=True, inferSchema= True)\n",
    "df.printSchema()\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|   education|count|\n",
      "+------------+-----+\n",
      "|   Preschool|   83|\n",
      "|     1st-4th|  247|\n",
      "|     5th-6th|  509|\n",
      "|   Doctorate|  594|\n",
      "|        12th|  657|\n",
      "|         9th|  756|\n",
      "| Prof-school|  834|\n",
      "|     7th-8th|  955|\n",
      "|        10th| 1389|\n",
      "|  Assoc-acdm| 1601|\n",
      "|        11th| 1812|\n",
      "|   Assoc-voc| 2061|\n",
      "|     Masters| 2657|\n",
      "|   Bachelors| 8025|\n",
      "|Some-college|10878|\n",
      "|     HS-grad|15784|\n",
      "+------------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|      native-country|count|\n",
      "+--------------------+-----+\n",
      "|  Holand-Netherlands|    1|\n",
      "|             Hungary|   19|\n",
      "|            Honduras|   20|\n",
      "|            Scotland|   21|\n",
      "|          Yugoslavia|   23|\n",
      "|Outlying-US(Guam-...|   23|\n",
      "|                Laos|   23|\n",
      "|     Trinadad&Tobago|   27|\n",
      "|            Cambodia|   28|\n",
      "|                Hong|   30|\n",
      "|            Thailand|   30|\n",
      "|             Ireland|   37|\n",
      "|              France|   38|\n",
      "|             Ecuador|   45|\n",
      "|                Peru|   46|\n",
      "|              Greece|   49|\n",
      "|           Nicaragua|   49|\n",
      "|                Iran|   59|\n",
      "|              Taiwan|   65|\n",
      "|            Portugal|   67|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"education\").count().sort(\"count\",ascending=True).show()\n",
    "df.groupBy(\"native-country\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|education|count|\n",
      "+---------+-----+\n",
      "|  5th-6th|  509|\n",
      "|  1st-4th|  247|\n",
      "|Preschool|   83|\n",
      "|Doctorate|  594|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnts = df.groupBy(\"education\").count()\n",
    "cnts.filter(cnts['count'] < 600).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|summary|                 x|               age|  workclass|            fnlwgt|   education|   educational-num|marital-status|      occupation|relationship|              race|gender|      capital-gain|     capital-loss|    hours-per-week|native-country|income|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "|  count|             48842|             48842|      48842|             48842|       48842|             48842|         48842|           48842|       48842|             48842| 48842|             48842|            48842|             48842|         48842| 48842|\n",
      "|   mean|           24421.5| 38.64358543876172|       null|189664.13459727284|        null|10.078088530363212|          null|            null|        null|              null|  null|1079.0676262233324|87.50231358257237|40.422382375824085|          null|  null|\n",
      "| stddev|14099.615260708357|13.710509934443525|       null|105604.02542315757|        null| 2.570972755592259|          null|            null|        null|              null|  null| 7452.019057655418| 403.004552124359|12.391444024252312|          null|  null|\n",
      "|    min|                 1|                17|          ?|             12285|        10th|                 1|      Divorced|               ?|     Husband|Amer-Indian-Eskimo|Female|                 0|                0|                 1|             ?| <=50K|\n",
      "|    max|             48842|                90|Without-pay|           1490400|Some-college|                16|       Widowed|Transport-moving|        Wife|             White|  Male|             99999|             4356|                99|    Yugoslavia|  >50K|\n",
      "+-------+------------------+------------------+-----------+------------------+------------+------------------+--------------+----------------+------------+------------------+------+------------------+-----------------+------------------+--------------+------+\n",
      "\n",
      "+----------+-----+----+\n",
      "|age_income|<=50K|>50K|\n",
      "+----------+-----+----+\n",
      "|        17|  595|   0|\n",
      "|        18|  862|   0|\n",
      "|        19| 1050|   3|\n",
      "|        20| 1112|   1|\n",
      "|        21| 1090|   6|\n",
      "|        22| 1161|  17|\n",
      "|        23| 1307|  22|\n",
      "|        24| 1162|  44|\n",
      "|        25| 1119|  76|\n",
      "|        26| 1068|  85|\n",
      "|        27| 1117| 115|\n",
      "|        28| 1101| 179|\n",
      "|        29| 1025| 198|\n",
      "|        30| 1031| 247|\n",
      "|        31| 1050| 275|\n",
      "|        32|  957| 296|\n",
      "|        33| 1045| 290|\n",
      "|        34|  949| 354|\n",
      "|        35|  997| 340|\n",
      "|        36|  948| 400|\n",
      "+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+------------------+\n",
      "|      marital-status| avg(capital-gain)|\n",
      "+--------------------+------------------+\n",
      "|           Separated| 581.8424836601307|\n",
      "|       Never-married|  384.382639449029|\n",
      "|Married-spouse-ab...| 629.0047770700637|\n",
      "|            Divorced| 793.6755615860094|\n",
      "|             Widowed| 603.6442687747035|\n",
      "|   Married-AF-spouse|2971.6216216216217|\n",
      "|  Married-civ-spouse|1739.7006121810625|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()\n",
    "df.crosstab('age', 'income').sort(\"age_income\").show()\n",
    "df.groupby('marital-status').agg({'capital-gain': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+\n",
      "|  x|age|workclass|fnlwgt|   education|educational-num|    marital-status|       occupation|relationship| race|gender|capital-gain|capital-loss|hours-per-week|native-country|income|age_square|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+\n",
      "|  1| 25|  Private|226802|        11th|              7|     Never-married|Machine-op-inspct|   Own-child|Black|  Male|           0|           0|            40| United-States| <=50K|     625.0|\n",
      "|  2| 38|  Private| 89814|     HS-grad|              9|Married-civ-spouse|  Farming-fishing|     Husband|White|  Male|           0|           0|            50| United-States| <=50K|    1444.0|\n",
      "|  3| 28|Local-gov|336951|  Assoc-acdm|             12|Married-civ-spouse|  Protective-serv|     Husband|White|  Male|           0|           0|            40| United-States|  >50K|     784.0|\n",
      "|  4| 44|  Private|160323|Some-college|             10|Married-civ-spouse|Machine-op-inspct|     Husband|Black|  Male|        7688|           0|            40| United-States|  >50K|    1936.0|\n",
      "|  5| 18|        ?|103497|Some-college|             10|     Never-married|                ?|   Own-child|White|Female|           0|           0|            30| United-States| <=50K|     324.0|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "age_square = df.select(F.col(\"age\")**2)\n",
    "# 2 Apply the transformation and add it to the DataFrame\n",
    "df = df.withColumn(\"age_square\",F.col(\"age\")**2)\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\n",
      "|  x|age|workclass|fnlwgt|   education|educational-num|    marital-status|       occupation|relationship| race|gender|capital-gain|capital-loss|hours-per-week|native-country|income|age_square|workclass_encoded|workclass_vec|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\n",
      "|  1| 25|  Private|226802|        11th|              7|     Never-married|Machine-op-inspct|   Own-child|Black|  Male|           0|           0|            40| United-States| <=50K|     625.0|              0.0|(9,[0],[1.0])|\n",
      "|  2| 38|  Private| 89814|     HS-grad|              9|Married-civ-spouse|  Farming-fishing|     Husband|White|  Male|           0|           0|            50| United-States| <=50K|    1444.0|              0.0|(9,[0],[1.0])|\n",
      "|  3| 28|Local-gov|336951|  Assoc-acdm|             12|Married-civ-spouse|  Protective-serv|     Husband|White|  Male|           0|           0|            40| United-States|  >50K|     784.0|              2.0|(9,[2],[1.0])|\n",
      "|  4| 44|  Private|160323|Some-college|             10|Married-civ-spouse|Machine-op-inspct|     Husband|Black|  Male|        7688|           0|            40| United-States|  >50K|    1936.0|              0.0|(9,[0],[1.0])|\n",
      "|  5| 18|        ?|103497|Some-college|             10|     Never-married|                ?|   Own-child|White|Female|           0|           0|            30| United-States| <=50K|     324.0|              3.0|(9,[3],[1.0])|\n",
      "+---+---+---------+------+------------+---------------+------------------+-----------------+------------+-----+------+------------+------------+--------------+--------------+------+----------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"workclass\", outputCol=\"workclass_encoded\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(dropLast=False, inputCol=\"workclass_encoded\", outputCol=\"workclass_vec\")\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "encoded.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example of creating an RDD from a distributed datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "rdd_csv = sc.wholeTextFiles(\"gs://<BUCKET_NAME>/*.csv\")\n",
    "rdd_csv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good examples of using JDBC DB connectors as source for DataFrames\n",
    "# https://github.com/tirthajyoti/Spark-with-Python/blob/master/Dataframe_SQL_query.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark1 = SparkSession.builder.appName('SQL').getOrCreate()\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"Data/chinook.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "\n",
    "tablename = \"artists\"\n",
    "\n",
    "df_artists = spark1.read.format(\"jdbc\").option(\"url\", url)/\n",
    ".option(\"dbtable\", tablename).option(\"driver\", driver).load()\n",
    "\n",
    "df_artists.show(5)\n",
    "\n",
    "spark1.sql(\"SELECT * FROM artists WHERE length(Name)<10 LIMIT 10\").show()\n",
    "# I guess I don't understand how the slark1 session gets tied to the DB connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
